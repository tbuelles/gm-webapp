from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from datetime import date
import json
import urllib.parse as p
import re
import os
import pickle

youtube_watch_url = 'https://www.youtube.com/watch?v='
SCOPES = ["https://www.googleapis.com/auth/youtube.force-ssl"]

def youtube_authenticate():
    os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'
    api_service_name = 'youtube'
    api_version = 'v3'
    client_secrets_file = 'credentials.json'
    creds = None
    # token.pickle stores access and refresh tokens
    # created automatically when the authorization flow completes
    # the first time
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    # if no credentials available, log in
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(client_secrets_file, SCOPES)
            creds = flow.run_local_server(port=0)
    # save credentials for next run
    with open('token.pickle', 'wb') as token:
        pickle.dump(creds, token)

    return build(api_service_name, api_version, credentials=creds)

# authenticate to YouTube API
youtube = youtube_authenticate()

# Functionality

def get_video_details(youtube, **kwargs):
    return youtube.videos().list(
        part="snippet,contentDetails,statistics",
        **kwargs
    ).execute()

def print_video_infos(video_response):
    items = video_response.get("items")[0]
    # get the snippet, statistics & content details from the video response
    snippet         = items["snippet"]
    statistics      = items["statistics"]
    content_details = items["contentDetails"]
    # get infos from the snippet
    channel_title = snippet["channelTitle"]
    title         = snippet["title"]
    description   = snippet["description"]
    publish_time  = snippet["publishedAt"]
    # get stats infos
    comment_count = statistics["commentCount"]
    like_count    = statistics["likeCount"]
    view_count    = statistics["viewCount"]
    # get duration from content details
    duration = content_details["duration"]
    # duration in the form of something like 'PT5H50M15S'
    # parsing it to be something like '5:50:15'
    parsed_duration = re.search(f"PT(\d+H)?(\d+M)?(\d+S)", duration).groups()
    duration_str = ""
    for d in parsed_duration:
        if d:
            duration_str += f"{d[:-1]}:"
    duration_str = duration_str.strip(":")
    print(f"""    Title: {title}
    Description: {description}
    Channel Title: {channel_title}
    Publish time: {publish_time}
    Duration: {duration_str}
    Number of comments: {comment_count}
    Number of likes: {like_count}
    Number of views: {view_count}
    """)

def search(youtube, **kwargs):
    return youtube.search().list(part='snippet',
                                **kwargs).execute()

def title_videoId(item):
    try:
        title = item['snippet']['title']
        videoId = item['id']['videoId']
        return (title, videoId)
    except KeyError:
        return ('QueryError', '')

# Query
def search_query(query, maxResults):
    response = search(youtube, q=query, maxResults=maxResults,
                 type='video')
    items = response.get('items')
    return [title_videoId(item) for item in items]

# read/ write urls.json
def update_url(key, query):
    with open('urls.json', 'r') as f:
        urls = json.load(f)
    title, videoId = search_query(query, maxResults=1)[0]
    new_url = 'https://www.youtube.com/embed/' + videoId
    urls[key] = new_url
    with open('urls.json', 'w') as f:
        json.dump(urls, f)

# Tagesschau 20:00 Uhr
def tagesschau():
    y = date.today() # - timedelta(days=1)
    y_str = y.strftime('%d.%m.%Y')
    key = 'tagesschau'
    query = 'tagesschau 20:00 Uhr' + ', ' + y_str
    update_url(key, query)
